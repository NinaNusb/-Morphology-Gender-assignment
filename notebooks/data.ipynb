{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "from typing import List, Tuple\n",
    "from spacy.language import Language\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../data/raw_scraped_data.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_df(df: pd.DataFrame)-> List[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Splits main df by 'lang' column, and creates new sub\n",
    "    dataFrames\n",
    "\n",
    "    returns:\n",
    "        list: pd.DataFrame\n",
    "    \"\"\"\n",
    "    languages = df['lang'].unique().tolist()\n",
    "    return [df[df['lang'] == lang] for lang in languages]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_filter(df: pd.DataFrame)-> pd.DataFrame:\n",
    "     \"\"\"\n",
    "     applies a filter to a df, then splits the df into smaller sub dfs \n",
    "     based on 'langauage' and further cleans them by dropping\n",
    "     andy duplicates found\n",
    "\n",
    "     returns:\n",
    "          pd.DataFrame\n",
    "     \"\"\"\n",
    "     filtered = df[(~df['noun'].str.contains('-| |\\.|1|2|3|4|5|6|7|8|9|0')) & (~df['noun'].str.isupper())]\n",
    "     dfs = split_df(filtered)\n",
    "     return pd.concat([df.drop_duplicates(subset=\"noun\", keep=False) for df in dfs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_lang(df: pd.DataFrame)-> List[Tuple[str, pd.DataFrame]]:\n",
    "    \"\"\"\n",
    "    Splits main df by 'lang' column, and creates new sub\n",
    "    dataFrames\n",
    "\n",
    "    returns:\n",
    "        list: namedtuple (lang, df)\n",
    "    \"\"\"\n",
    "    Sub_df = namedtuple('Sub_df', ['lang', 'df'])\n",
    "    languages = df['lang'].unique().tolist()\n",
    "    dataframes = split_df(df)\n",
    "    return [Sub_df(lang, sub_df) for lang, sub_df in zip(languages, dataframes)]\n",
    "\n",
    "\n",
    "def sub_df_and_model(df: pd.DataFrame)-> List[Tuple[pd.DataFrame, Language]]:\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    Model = namedtuple('Model', ['lang', 'nlp'])\n",
    "    Df_nlp = namedtuple('Df_and_Model', ['df', 'nlp'])\n",
    "    sub_dfs = df_lang(df) \n",
    "    d = {'Spanish': 'es', 'French': 'fr', 'German': 'de', 'Polish': 'pl'}\n",
    "    models = [Model(lang, spacy.load(lang + '_core_news_sm')) for lang in d.values()]\n",
    "    return [Df_nlp(sub_df.df, model.nlp) for sub_df, model in zip(sub_dfs, models)]\n",
    "\n",
    "\n",
    "def add_lemma(tup: Tuple[List[str], Language])-> List[str]:\n",
    "    \"\"\"\n",
    "    takes in a namedtuple, uses the list of nouns stored in tup.words\n",
    "    and passes each word into SpaCy POS tagger, appending only the nouns\n",
    "    NOT labeled as Proper Nouns\n",
    "\n",
    "    returns:\n",
    "        list: nouns (str)\n",
    "    \"\"\"\n",
    "    nlp = tup.nlp \n",
    "    text = \" \".join(tup.words) # all nouns from list into a str\n",
    "    nlp.max_length = len(text) # increase the length the parser can handle\n",
    "    doc = nlp(text) \n",
    "    data = [(token.text, token.lemma_) for token in doc if token.pos_ != 'PROPN']\n",
    "    words = [word[0] for word in data]\n",
    "    lemma_df = pd.DataFrame(data, columns=['noun', 'lemma'])\n",
    "    filtered_df = tup.df[tup.df['noun'].isin(words)]\n",
    "    return pd.merge(lemma_df, filtered_df, on=['noun'], how='inner')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_df(df: pd.DataFrame)-> List[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    takes in a DataFrame, creates sub dataframes based on each unique language,\n",
    "    then takes each word found in each sub dataframe and passes it into SpaCy\n",
    "    POS tagger and filters out nouns NOT labeled as Proper Nouns, utlimately\n",
    "    return a list of sub dataframes complelety populated by nouns in each\n",
    "    given language.\n",
    "\n",
    "    returns:\n",
    "        res(list): list of sub dataframes per language\n",
    "    \"\"\"\n",
    "    Data = namedtuple('Data', ['words', 'nlp', 'df'])\n",
    "    df_and_nlp = sub_df_and_model(df) # sub dataFrames and spacy nlp models\n",
    "    res = [] \n",
    "    for tup in df_and_nlp:\n",
    "        data = Data(pd.Series(df['noun']).tolist(), tup.nlp, df) # create a Data namedtuple (list of nouns, specific language model)\n",
    "        res.append(add_lemma(data))\n",
    "    return pd.concat(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def raw_json_to_clean_df(path):\n",
    "    filtered_df = initial_filter(pd.read_json(path))\n",
    "    return clean_df(filtered_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = raw_json_to_clean_df(path)\n",
    "df.to_csv('../data/cleaned_data.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "-Morphology-Gender-assignment-lrLn6GZY",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
