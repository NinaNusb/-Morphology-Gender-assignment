{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, '../src/')\n",
    "from data_cleaning import raw_json_to_clean_df\n",
    "from data_transformation import transform_clean_df, get_X_y, get_dist\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import Perceptron\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Collection\n",
    "\n",
    "We begin by scrapping Wiktionary.org for *feminine*, *masculine*, and *neuter* nouns in *Polish*, *German*, *Spanish*, and *French*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run webscrapper"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Cleaning\n",
    "\n",
    "The raw data in json format must be cleaned: removing nouns with *spaces*, *hyphens*, *numbers*, *abbreviations*, *initials* and finally those that are *proper nouns*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'>' not supported between instances of 'method' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# read json file and load it as a DataFrame\u001b[39;00m\n\u001b[1;32m      2\u001b[0m path \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m../data/raw_scraped_data.json\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> 3\u001b[0m df \u001b[39m=\u001b[39m raw_json_to_clean_df(path)\n",
      "File \u001b[0;32m~/Uni/S2/Ad Morpho/Project/-Morphology-Gender-assignment/notebooks/../src/data_cleaning.py:111\u001b[0m, in \u001b[0;36mraw_json_to_clean_df\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mraw_json_to_clean_df\u001b[39m(path: \u001b[39mstr\u001b[39m)\u001b[39m-\u001b[39m\u001b[39m>\u001b[39m pd\u001b[39m.\u001b[39mDataFrame:\n\u001b[1;32m    104\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[39m    reads in raw data json file , applies filters, \u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[39m    and returns a final and clean df to be used on an NLP model\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[39m        pd.DataFrame\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 111\u001b[0m     filtered_df \u001b[39m=\u001b[39m initial_filter(pd\u001b[39m.\u001b[39;49mread_json(path))\n\u001b[1;32m    112\u001b[0m     \u001b[39mreturn\u001b[39;00m clean_df(filtered_df)\n",
      "File \u001b[0;32m~/Uni/S2/Ad Morpho/Project/-Morphology-Gender-assignment/notebooks/../src/data_cleaning.py:29\u001b[0m, in \u001b[0;36minitial_filter\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minitial_filter\u001b[39m(df: pd\u001b[39m.\u001b[39mDataFrame)\u001b[39m-\u001b[39m\u001b[39m>\u001b[39m pd\u001b[39m.\u001b[39mDataFrame:\n\u001b[1;32m     21\u001b[0m \u001b[39m     \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[39m     applies a filter to a df, then splits the df into smaller sub dfs \u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[39m     based on 'langauage' and further cleans them by dropping\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[39m          pd.DataFrame\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[39m     \"\"\"\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m      filtered \u001b[39m=\u001b[39m df[(\u001b[39m~\u001b[39mdf[\u001b[39m'\u001b[39m\u001b[39mnoun\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mstr\u001b[39m.\u001b[39mcontains(\u001b[39m'\u001b[39m\u001b[39m-| |\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m.|1|2|3|4|5|6|7|8|9|0\u001b[39m\u001b[39m'\u001b[39m)) \u001b[39m&\u001b[39m (\u001b[39m~\u001b[39mdf[\u001b[39m'\u001b[39m\u001b[39mnoun\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mstr\u001b[39m.\u001b[39misupper()) \u001b[39m&\u001b[39m (df[\u001b[39m'\u001b[39;49m\u001b[39mnoun\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mstr\u001b[39m.\u001b[39;49mlen \u001b[39m>\u001b[39;49m \u001b[39m3\u001b[39;49m)]\n\u001b[1;32m     30\u001b[0m      dfs \u001b[39m=\u001b[39m split_df(filtered)\n\u001b[1;32m     31\u001b[0m      \u001b[39mreturn\u001b[39;00m pd\u001b[39m.\u001b[39mconcat([df\u001b[39m.\u001b[39mdrop_duplicates(subset\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mnoun\u001b[39m\u001b[39m\"\u001b[39m, keep\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m) \u001b[39mfor\u001b[39;00m df \u001b[39min\u001b[39;00m dfs])\n",
      "\u001b[0;31mTypeError\u001b[0m: '>' not supported between instances of 'method' and 'int'"
     ]
    }
   ],
   "source": [
    "# read json file and load it as a DataFrame\n",
    "path = '../data/raw_scraped_data.json'\n",
    "df = raw_json_to_clean_df(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>noun</th>\n",
       "      <th>lemma</th>\n",
       "      <th>gender</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aalen</td>\n",
       "      <td>aalir</td>\n",
       "      <td>masculine</td>\n",
       "      <td>Polish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abacysta</td>\n",
       "      <td>abacysta</td>\n",
       "      <td>masculine</td>\n",
       "      <td>Polish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abatacept</td>\n",
       "      <td>abatacept</td>\n",
       "      <td>masculine</td>\n",
       "      <td>Polish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abatacept</td>\n",
       "      <td>abatacept</td>\n",
       "      <td>masculine</td>\n",
       "      <td>French</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abatacept</td>\n",
       "      <td>abatacept</td>\n",
       "      <td>masculine</td>\n",
       "      <td>Polish</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        noun      lemma     gender    lang\n",
       "0      aalen      aalir  masculine  Polish\n",
       "1   abacysta   abacysta  masculine  Polish\n",
       "2  abatacept  abatacept  masculine  Polish\n",
       "3  abatacept  abatacept  masculine  French\n",
       "4  abatacept  abatacept  masculine  Polish"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Transform and Encode Data\n",
    "reduce data down to an even amount of examples per language and per gender\n",
    "encode last 3 letters of each noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/capeta/Uni/S2/Ad Morpho/Project/-Morphology-Gender-assignment/notebooks/../src/data/data_transformation.py:14: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
      "  reduced_df = df.groupby(['lang', 'gender'])['noun', 'lemma', 'gender', 'lang'].sample(n=3000)\n",
      "/home/capeta/.local/share/virtualenvs/-Morphology-Gender-assignment-lrLn6GZY/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>noun</th>\n",
       "      <th>lemma</th>\n",
       "      <th>gender</th>\n",
       "      <th>lang</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>...</th>\n",
       "      <th>2445</th>\n",
       "      <th>2446</th>\n",
       "      <th>2447</th>\n",
       "      <th>2448</th>\n",
       "      <th>2449</th>\n",
       "      <th>2450</th>\n",
       "      <th>2451</th>\n",
       "      <th>2452</th>\n",
       "      <th>2453</th>\n",
       "      <th>2454</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cafétérie</td>\n",
       "      <td>cafétéria</td>\n",
       "      <td>feminine</td>\n",
       "      <td>French</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>iconographie</td>\n",
       "      <td>iconographie</td>\n",
       "      <td>feminine</td>\n",
       "      <td>French</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>oniomanie</td>\n",
       "      <td>oniomanie</td>\n",
       "      <td>feminine</td>\n",
       "      <td>French</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>baïonnette</td>\n",
       "      <td>baïonnette</td>\n",
       "      <td>feminine</td>\n",
       "      <td>French</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Walcourt</td>\n",
       "      <td>Walcourt</td>\n",
       "      <td>feminine</td>\n",
       "      <td>French</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2459 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           noun         lemma    gender    lang    0    1    2    3    4    5  \\\n",
       "0     cafétérie     cafétéria  feminine  French  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "1  iconographie  iconographie  feminine  French  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "2     oniomanie     oniomanie  feminine  French  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "3    baïonnette    baïonnette  feminine  French  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "4      Walcourt      Walcourt  feminine  French  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "   ...  2445  2446  2447  2448  2449  2450  2451  2452  2453  2454  \n",
       "0  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "1  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "2  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "3  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "4  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "\n",
       "[5 rows x 2459 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_df = transform_clean_df(df)\n",
    "trans_df.head(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Train Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Define X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Split data into training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 K-NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8421666666666666"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(X_train, y_train)\n",
    "knn.score(X_test, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8773333333333333"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = Perceptron(random_state=42)\n",
    "p.fit(X_train, y_train)\n",
    "p.score(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
