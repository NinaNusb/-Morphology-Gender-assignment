{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "from typing import List, Tuple\n",
    "from spacy.language import Language\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read json file and load it as a DataFrame\n",
    "df = pd.read_json('../data/raw_scraped_data.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intial filtering of hypen, blank space, all digits and all caps words\n",
    "filtered = df[(~df['noun'].str.contains('-| |\\.|1|2|3|4|5|6|7|8|9|0')) & (~df['noun'].str.isupper())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataframe into smaller ones by language\n",
    "def split_df(df: pd.DataFrame)-> List[Tuple[str, pd.DataFrame]]:\n",
    "    \"\"\"\n",
    "    Splits main df by 'lang' column, and creates new sub\n",
    "    dataFrames\n",
    "\n",
    "    returns:\n",
    "        list: namedtuple (lang, df)\n",
    "    \"\"\"\n",
    "    Sub_df = namedtuple('Sub_df', ['lang', 'df'])\n",
    "    languages = df['lang'].unique().tolist()\n",
    "    dataframes = [df[df['lang'] == lang] for lang in languages]\n",
    "    return [Sub_df(lang, sub_df) for lang, sub_df in zip(languages, dataframes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# match sub dataFrames with its Spacy language model respectively\n",
    "def sub_df_and_model(df: pd.DataFrame)-> List[Tuple[pd.DataFrame, Language]]:\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    Model = namedtuple('Model', ['lang', 'nlp'])\n",
    "    Df_nlp = namedtuple('Df_and_Model', ['df', 'nlp'])\n",
    "    sub_dfs = split_df(df) # list of all sub DataFrames based on language\n",
    "    d = {'Spanish': 'es', 'French': 'fr', 'German': 'de', 'Polish': 'pl'}\n",
    "    models = [Model(lang, spacy.load(lang + '_core_news_sm')) for lang in d.values()]\n",
    "    return [Df_nlp(sub_df.df, model.nlp) for sub_df, model in zip(sub_dfs, models)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def good_pos_list(tup: Tuple[List[str], Language])-> List[str]:\n",
    "    \"\"\"\n",
    "    takes in a namedtuple, uses the list of nouns stored in tup.words\n",
    "    and passes each word into SpaCy POS tagger, appending only the nouns\n",
    "    NOT labeled as Proper Nouns\n",
    "\n",
    "    returns:\n",
    "        list: nouns (str)\n",
    "    \"\"\"\n",
    "    nlp = tup.nlp\n",
    "    text = \" \".join(tup.words) # all nouns from list into a str\n",
    "    nlp.max_length = len(text) # increase the length the parser can handle\n",
    "    doc = nlp(text) \n",
    "    return [token.text for token in doc if token.pos_ != 'PROPN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_df(df: pd.DataFrame)-> List[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    takes in a DataFrame, creates sub dataframes based on each unique language,\n",
    "    then takes each word found in each sub dataframe and passes it into SpaCy\n",
    "    POS tagger and filters out nouns NOT labeled as Proper Nouns, utlimately\n",
    "    return a list of sub dataframes complelety populated by nouns in each\n",
    "    given language.\n",
    "\n",
    "    returns:\n",
    "        res(list): list of sub dataframes per language\n",
    "    \"\"\"\n",
    "    Data = namedtuple('Data', ['words', 'nlp'])\n",
    "    df_and_nlp = sub_df_and_model(df) # sub dataFrames and spacy nlp models\n",
    "    res = [] # empty list to hold results\n",
    "    for tup in df_and_nlp: # for every tuple (sub DataFrame, spacy model)\n",
    "        data = Data(pd.Series(tup.df['noun']).tolist(), tup.nlp) # create a Data namedtuple (list of nouns, specific language model)\n",
    "        res.append(tup.df[tup.df['noun'].isin(good_pos_list(data))]) # append a sub DataFrame per language with nouns verified as non-proper nouns via SpaCy\n",
    "    return res # return the new list of sub dataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sub_to_json(sub_dfs: List[pd.DataFrame])-> None:\n",
    "    \"\"\"\n",
    "    takes a list of sub dataFrames and creates a json for each\n",
    "    the name of the file is modified by the particular language the sub DataFrame\n",
    "    represents\n",
    "    \"\"\"\n",
    "    base = '../data/'\n",
    "    filename = '_cleaned_data.json'\n",
    "    for df in sub_dfs:\n",
    "        lang = df['lang'].unique().tolist()[0] # get only value in column 'lang'\n",
    "        df.to_json(base + lang + filename, orient='split')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
