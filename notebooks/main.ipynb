{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /home/capeta/.local/share/virtualenvs/-Morphology-Gender-assignment-lrLn6GZY/lib/python3.10/site-packages (3.5.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/capeta/.local/share/virtualenvs/-Morphology-Gender-assignment-lrLn6GZY/lib/python3.10/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/capeta/.local/share/virtualenvs/-Morphology-Gender-assignment-lrLn6GZY/lib/python3.10/site-packages (from spacy) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/capeta/.local/share/virtualenvs/-Morphology-Gender-assignment-lrLn6GZY/lib/python3.10/site-packages (from spacy) (1.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/capeta/.local/share/virtualenvs/-Morphology-Gender-assignment-lrLn6GZY/lib/python3.10/site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/capeta/.local/share/virtualenvs/-Morphology-Gender-assignment-lrLn6GZY/lib/python3.10/site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /home/capeta/.local/share/virtualenvs/-Morphology-Gender-assignment-lrLn6GZY/lib/python3.10/site-packages (from spacy) (8.1.9)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/capeta/.local/share/virtualenvs/-Morphology-Gender-assignment-lrLn6GZY/lib/python3.10/site-packages (from spacy) (1.1.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/capeta/.local/share/virtualenvs/-Morphology-Gender-assignment-lrLn6GZY/lib/python3.10/site-packages (from spacy) (2.4.6)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/capeta/.local/share/virtualenvs/-Morphology-Gender-assignment-lrLn6GZY/lib/python3.10/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /home/capeta/.local/share/virtualenvs/-Morphology-Gender-assignment-lrLn6GZY/lib/python3.10/site-packages (from spacy) (0.7.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /home/capeta/.local/share/virtualenvs/-Morphology-Gender-assignment-lrLn6GZY/lib/python3.10/site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/capeta/.local/share/virtualenvs/-Morphology-Gender-assignment-lrLn6GZY/lib/python3.10/site-packages (from spacy) (6.3.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/capeta/.local/share/virtualenvs/-Morphology-Gender-assignment-lrLn6GZY/lib/python3.10/site-packages (from spacy) (4.65.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/capeta/.local/share/virtualenvs/-Morphology-Gender-assignment-lrLn6GZY/lib/python3.10/site-packages (from spacy) (1.24.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/capeta/.local/share/virtualenvs/-Morphology-Gender-assignment-lrLn6GZY/lib/python3.10/site-packages (from spacy) (2.28.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /home/capeta/.local/share/virtualenvs/-Morphology-Gender-assignment-lrLn6GZY/lib/python3.10/site-packages (from spacy) (1.10.7)\n",
      "Requirement already satisfied: jinja2 in /home/capeta/.local/share/virtualenvs/-Morphology-Gender-assignment-lrLn6GZY/lib/python3.10/site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /home/capeta/.local/share/virtualenvs/-Morphology-Gender-assignment-lrLn6GZY/lib/python3.10/site-packages (from spacy) (67.6.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/capeta/.local/share/virtualenvs/-Morphology-Gender-assignment-lrLn6GZY/lib/python3.10/site-packages (from spacy) (23.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/capeta/.local/share/virtualenvs/-Morphology-Gender-assignment-lrLn6GZY/lib/python3.10/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /home/capeta/.local/share/virtualenvs/-Morphology-Gender-assignment-lrLn6GZY/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/capeta/.local/share/virtualenvs/-Morphology-Gender-assignment-lrLn6GZY/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/capeta/.local/share/virtualenvs/-Morphology-Gender-assignment-lrLn6GZY/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/capeta/.local/share/virtualenvs/-Morphology-Gender-assignment-lrLn6GZY/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/capeta/.local/share/virtualenvs/-Morphology-Gender-assignment-lrLn6GZY/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/capeta/.local/share/virtualenvs/-Morphology-Gender-assignment-lrLn6GZY/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/capeta/.local/share/virtualenvs/-Morphology-Gender-assignment-lrLn6GZY/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.0.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/capeta/.local/share/virtualenvs/-Morphology-Gender-assignment-lrLn6GZY/lib/python3.10/site-packages (from typer<0.8.0,>=0.3.0->spacy) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/capeta/.local/share/virtualenvs/-Morphology-Gender-assignment-lrLn6GZY/lib/python3.10/site-packages (from jinja2->spacy) (2.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, '../src/')\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from collections import defaultdict\n",
    "# from data_cleaning import split_df, df_lang\n",
    "from data_transformation import get_X_y, distribution\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.preprocessing import OneHotEncoder\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Collection\n",
    "\n",
    "We begin by scrapping Wiktionary.org for *feminine*, *masculine*, and *neuter* nouns in *Polish*, *German*, *Spanish*, and *French*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run webscrapper"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Cleaning\n",
    "\n",
    "The raw data in json format must be cleaned: removing nouns with *spaces*, *hyphens*, *numbers*, *abbreviations*, *initials* and finally those that are *proper nouns*."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>noun</th>\n",
       "      <th>gender</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>masculine</td>\n",
       "      <td>Polish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Aalborg</td>\n",
       "      <td>masculine</td>\n",
       "      <td>Polish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aalen</td>\n",
       "      <td>masculine</td>\n",
       "      <td>Polish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Aarhus</td>\n",
       "      <td>masculine</td>\n",
       "      <td>Polish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abacysta</td>\n",
       "      <td>masculine</td>\n",
       "      <td>Polish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328184</th>\n",
       "      <td>zurrona</td>\n",
       "      <td>feminine</td>\n",
       "      <td>Spanish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328185</th>\n",
       "      <td>zutana</td>\n",
       "      <td>feminine</td>\n",
       "      <td>Spanish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328186</th>\n",
       "      <td>zwingliana</td>\n",
       "      <td>feminine</td>\n",
       "      <td>Spanish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328187</th>\n",
       "      <td>zwingliana</td>\n",
       "      <td>feminine</td>\n",
       "      <td>Spanish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328188</th>\n",
       "      <td>zwingliana</td>\n",
       "      <td>feminine</td>\n",
       "      <td>Spanish</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>328189 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              noun     gender     lang\n",
       "0                a  masculine   Polish\n",
       "1          Aalborg  masculine   Polish\n",
       "2            aalen  masculine   Polish\n",
       "3           Aarhus  masculine   Polish\n",
       "4         abacysta  masculine   Polish\n",
       "...            ...        ...      ...\n",
       "328184     zurrona   feminine  Spanish\n",
       "328185      zutana   feminine  Spanish\n",
       "328186  zwingliana   feminine  Spanish\n",
       "328187  zwingliana   feminine  Spanish\n",
       "328188  zwingliana   feminine  Spanish\n",
       "\n",
       "[328189 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read json file and load it as a DataFrame\n",
    "path = '../data/raw_scraped_data.json'\n",
    "raw_df = pd.read_json(path)\n",
    "raw_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Initial filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(301257, 3)\n",
      "(299192, 3)\n"
     ]
    }
   ],
   "source": [
    "# remove numbers, hypens, spaces, and periods\n",
    "no_nums = raw_df[(~raw_df['noun'].str.contains('-| |\\.|1|2|3|4|5|6|7|8|9|0'))]\n",
    "print(no_nums.shape)\n",
    "# remove full uppercase\n",
    "no_caps = no_nums[(~no_nums['noun'].str.isupper())]\n",
    "print(no_caps.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Further cleaning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Break df into each language to: \n",
    "- remove duplicates in each language\n",
    "- remove nouns that begin with a capital letter (except for German) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "from spacy.language import Language\n",
    "from collections import namedtuple\n",
    "\n",
    "\n",
    "def df_lang(df: pd.DataFrame)-> List[Tuple[str, pd.DataFrame]]:\n",
    "    \"\"\"\n",
    "    Splits main df by 'lang' column, and creates new sub\n",
    "    dataFrames\n",
    "\n",
    "    returns:\n",
    "        list: namedtuple (lang, df)\n",
    "    \"\"\"\n",
    "    Sub_df = namedtuple('Sub_df', ['lang', 'df'])\n",
    "    languages = df['lang'].unique()\n",
    "    dataframes = [df[df['lang'] == lang] for lang in languages]\n",
    "    return [Sub_df(lang, sub_df) for lang, sub_df in zip(languages, dataframes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.load(\"de_core_news_sm\")\n",
    "\n",
    "# separate df for each language\n",
    "dfs = df_lang(no_caps)\n",
    "\n",
    "# remove nouns that begin with captial letters (except German)\n",
    "temp = []\n",
    "for sub_df in dfs:\n",
    "    if sub_df.lang != 'German':\n",
    "        temp.append(sub_df.df[~sub_df.df['noun'].str.istitle()])\n",
    "    else: # otherwise, for German, pass into SpaCy and filter out Proper nouns\n",
    "        words = pd.Series(sub_df.df['noun']).tolist()\n",
    "        nlp = spacy.load(\"de_core_news_sm\")\n",
    "        text = \" \".join(words) \n",
    "        nlp.max_length = len(text) \n",
    "        doc = nlp(text) \n",
    "        tokens = [token.text for token in doc if token.pos_ != 'PROPN']\n",
    "        temp.append(sub_df.df[sub_df.df['noun'].isin(tokens)])\n",
    "clean = pd.concat(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>noun</th>\n",
       "      <th>gender</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>masculine</td>\n",
       "      <td>Polish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aalen</td>\n",
       "      <td>masculine</td>\n",
       "      <td>Polish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abacysta</td>\n",
       "      <td>masculine</td>\n",
       "      <td>Polish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>abak</td>\n",
       "      <td>masculine</td>\n",
       "      <td>Polish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>abakawir</td>\n",
       "      <td>masculine</td>\n",
       "      <td>Polish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328184</th>\n",
       "      <td>zurrona</td>\n",
       "      <td>feminine</td>\n",
       "      <td>Spanish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328185</th>\n",
       "      <td>zutana</td>\n",
       "      <td>feminine</td>\n",
       "      <td>Spanish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328186</th>\n",
       "      <td>zwingliana</td>\n",
       "      <td>feminine</td>\n",
       "      <td>Spanish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328187</th>\n",
       "      <td>zwingliana</td>\n",
       "      <td>feminine</td>\n",
       "      <td>Spanish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328188</th>\n",
       "      <td>zwingliana</td>\n",
       "      <td>feminine</td>\n",
       "      <td>Spanish</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>261970 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              noun     gender     lang\n",
       "0                a  masculine   Polish\n",
       "2            aalen  masculine   Polish\n",
       "4         abacysta  masculine   Polish\n",
       "5             abak  masculine   Polish\n",
       "7         abakawir  masculine   Polish\n",
       "...            ...        ...      ...\n",
       "328184     zurrona   feminine  Spanish\n",
       "328185      zutana   feminine  Spanish\n",
       "328186  zwingliana   feminine  Spanish\n",
       "328187  zwingliana   feminine  Spanish\n",
       "328188  zwingliana   feminine  Spanish\n",
       "\n",
       "[261970 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Transform and Encode data\n",
    "#### 3.1 Create function to add # to all nouns\n",
    "Make all nouns the same length, this is done by preprending '#' to nouns until they are a uniform length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     noun     gender    lang\n",
      "0  #####################################a  masculine  Polish\n",
      "2  #################################aalen  masculine  Polish\n",
      "4  ##############################abacysta  masculine  Polish\n",
      "5  ##################################abak  masculine  Polish\n",
      "7  ##############################abakawir  masculine  Polish\n"
     ]
    }
   ],
   "source": [
    "# get length of longest noun\n",
    "max_length = clean['noun'].str.len().max() # 38\n",
    "\n",
    "def add_filler(word):\n",
    "    \"\"\" preprends n amount #'s to a word \n",
    "    based on a max_length\"\"\"\n",
    "    if len(word) < max_length: # if word len() is less than the max len()\n",
    "        diff = max_length - len(word) # we subtract the current word len() by the max len(): diff\n",
    "        return '#' * diff + word # we prepend n(diff) amount of '#'s to the word and then return it\n",
    "    return word # if len() of word is NOT less then the max len, then just return it\n",
    "\n",
    "# apply function to every value in column 'noun'\n",
    "clean['noun'] = clean['noun'].apply(add_filler)\n",
    "print(clean.head(5))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 \n",
    "get a distribution of all languages and genders and get the lowest count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lang       French  German  Polish   Spanish\n",
      "gender                                     \n",
      "feminine   3801.0  3624.0  2555.0  116375.0\n",
      "masculine  3911.0  2485.0  2681.0  119263.0\n",
      "neuter        NaN  2392.0  4883.0       NaN\n",
      "lowest_value =  2392\n"
     ]
    }
   ],
   "source": [
    "grouped = clean.groupby(['gender','lang']).size().unstack()\n",
    "lowest_value = int(grouped.min().min())\n",
    "print(distribution(clean))\n",
    "print('lowest_value = ', lowest_value)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Reduce all language sub dfs and genders to a uniform amount (lowest_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lang       French  German  Polish  Spanish\n",
      "gender                                    \n",
      "feminine   2392.0  2392.0  2392.0   2392.0\n",
      "masculine  2392.0  2392.0  2392.0   2392.0\n",
      "neuter        NaN  2392.0  2392.0      NaN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_55076/1350974568.py:1: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
      "  reduced_df = clean.groupby(['lang', 'gender'])['noun', 'gender', 'lang'].sample(n=lowest_value) # reduce each language and gender by lowest_value\n"
     ]
    }
   ],
   "source": [
    "reduced_df = clean.groupby(['lang', 'gender'])['noun', 'gender', 'lang'].sample(n=lowest_value) # reduce each language and gender by lowest_value\n",
    "print(distribution(reduced_df))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Define an encoding function\n",
    "aim is to grab n amount of letters in a noun (starting from the end), and passing it into a one-hot encoder that will represent said letter(s) as a vector.\n",
    "1. grab n amount of letters from each noun (this will be encoded)\n",
    "2. intialize an encoder\n",
    "3. encode n amount of letters\n",
    "4. convert results into a dataframe (transformed df)\n",
    "5. re index reduced df\n",
    "6. return new dataframe which is made of : reduced df and transformed df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(reduced_df, n=0):\n",
    "    to_be_encoded = reduced_df['noun'].str[-n:] # grab n amount of letters start from the end to encode only\n",
    "    ohe = OneHotEncoder(sparse_output=False) # initialize the encoder\n",
    "    transformed = ohe.fit_transform(to_be_encoded.to_numpy().reshape(-1, 1)) # encode\n",
    "    transformed_df = pd.DataFrame(transformed) # convert to a dataframe\n",
    "    reduced_df.reset_index(inplace=True, drop=True) # reset indexes\n",
    "    return pd.concat([reduced_df, transformed_df], axis=1) # create new dataframe of reduced df and transformed df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Train, test, fit, score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 create function to train,fit, test all languages with n amount of encoding\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_train_per_lang(df):\n",
    "    results = defaultdict(lambda :defaultdict(list)) # to store scores from ML Models. dict->dict->list\n",
    "    max_length = df['noun'].str.len().max() # get longest noun in whole dataset\n",
    "    dfs = df_lang(df) # break df into smalled dfs based on language: spanish_df, french_df etc\n",
    "    for sub_df in dfs: # for each language df\n",
    "        for n in range(1, max_length + 1): # \n",
    "            encoded_df = encode(sub_df.df, n) # encode n amount of letters for the ith langauge df\n",
    "            X, y = get_X_y(encoded_df) # X is the vector representationf for n amount of letters, y are the labels\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "            # now that we have a train test split, we can plug them into our ML models\n",
    "            # KNN\n",
    "            knn = KNeighborsClassifier(n_neighbors=3) # initialize a KNN class, 3 neighbors\n",
    "            knn.fit(X_train, y_train) # train it \n",
    "            results['KNN'][n].append((knn.score(X_test, y_test), sub_df.lang)) # append score into results dict, along with name of sub df (French, German, etc)\n",
    "\n",
    "            # Perceptron\n",
    "            p = Perceptron(random_state=42) # initialize a Perceptron class, random state 42\n",
    "            p.fit(X_train, y_train) # train it\n",
    "            results['Perceptron'][n].append((p.score(X_test, y_test), sub_df.lang)) # append score to results dict, along with name of sub df (French, German, etc)\n",
    "\n",
    "    return results # return s scores of l sub dfs in df, of n amount of letters encode, for c amount of ML models. Ex: results[c][n]: (s,l)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Experiment 1: train and test each language individually on n amount of letters encoded \n",
    "## takes ~5 mins to run!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_langs = multi_train_per_lang(reduced_df)\n",
    "# all_langs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Show results for KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# knn_results = all_langs['KNN']\n",
    "# knn_df = pd.DataFrame.from_dict(knn_results, orient=\"index\", columns=['Polish', 'German', 'French', 'Spanish'])\n",
    "# knn_iterative_df = knn_df.applymap(lambda x: x[0]) # grab first element in the tuple (score, langauage)\n",
    "# knn_iterative_df.to_csv('../data/knn_per_language_results.csv') # save to csv\n",
    "# knn_iterative_df # display dataframe"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Show results for Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perceptron_results = all_langs['Perceptron']\n",
    "# per_df = pd.DataFrame.from_dict(perceptron_results, orient=\"index\", columns=['Polish', 'German', 'French', 'Spanish'])\n",
    "# per_iterative_df = per_df.applymap(lambda x: x[0]) # grab first element in the tuple (score, langauage)\n",
    "# per_iterative_df.to_csv('../data/perceptron_per_language_results.csv') # save as csv\n",
    "# per_iterative_df # display df\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Expermiment 2: train on one lang, and test on a different langauage (shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_train_per_lang_shuffle(df, max_length):\n",
    "    results = defaultdict(lambda :defaultdict(list))\n",
    "    dfs = df_lang(df)\n",
    "    training_data = defaultdict()\n",
    "    testing_data = defaultdict()\n",
    "    for sub_df in dfs: # for each language\n",
    "        trans_df = encode(sub_df.df, max_length) # encode\n",
    "        X, y = get_X_y(trans_df) # get X and y\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5) # split\n",
    "        training_data[sub_df.lang] = (X_train, y_train) # add the x and y train for each language into a dict\n",
    "        testing_data[sub_df.lang] = (X_test, y_test) # add the x and y testing for each language into a dict\n",
    "        \n",
    "    for train_lang in training_data: # for every language training\n",
    "        for test_lang in testing_data: # for every language testing\n",
    "       \n",
    "            X_train__, y_train__ = training_data[train_lang]\n",
    "            X_test__, y_test__ = testing_data[test_lang]\n",
    "            try:\n",
    "                knn = KNeighborsClassifier(n_neighbors=3) # initialize a KNN\n",
    "                knn.fit(X_train__, y_train__) # fit it with training\n",
    "                results['KNN'][train_lang] = (knn.score(X_test__, y_test__), test_lang ) # test it\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                p = Perceptron(random_state=42) # initialize a Perceptron\n",
    "                p.fit(X_train__, y_train__) # fit it with training\n",
    "                results['Perceptron'][train_lang] = (p.score(X_test__, y_test__), test_lang ) # test it\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    return results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Run experiment on only 4 letters encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.multi_train_per_lang_shuffle.<locals>.<lambda>()>,\n",
       "            {'KNN': defaultdict(list,\n",
       "                         {'French': (0.6864548494983278, 'French'),\n",
       "                          'German': (0.7644927536231884, 'German'),\n",
       "                          'Polish': (0.7198996655518395, 'Polish'),\n",
       "                          'Spanish': (0.6513377926421404, 'Spanish')}),\n",
       "             'Perceptron': defaultdict(list,\n",
       "                         {'French': (0.7976588628762542, 'French'),\n",
       "                          'German': (0.7920847268673356, 'German'),\n",
       "                          'Polish': (0.8678929765886287, 'Polish'),\n",
       "                          'Spanish': (0.7190635451505016, 'Spanish')})})"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shuffled = multi_train_per_lang_shuffle(reduced_df, 4)\n",
    "shuffled\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7 show results of KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>French</th>\n",
       "      <th>German</th>\n",
       "      <th>Polish</th>\n",
       "      <th>Spanish</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>French</th>\n",
       "      <td>0.686455</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>German</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.764493</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Polish</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7199</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Spanish</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.651338</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           French    German  Polish   Spanish\n",
       "French   0.686455       NaN     NaN       NaN\n",
       "German        NaN  0.764493     NaN       NaN\n",
       "Polish        NaN       NaN  0.7199       NaN\n",
       "Spanish       NaN       NaN     NaN  0.651338"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_res = shuffled['KNN']\n",
    "data = {key: [result[0] if result[1] == key else None for result in knn_res.values()] for key in knn_res.keys()}\n",
    "knn_res_df = pd.DataFrame(data, index=data.keys())\n",
    "knn_res_df.to_csv('../data/knn_res_lang_shuffle_4_encoding.csv') # save as csv\n",
    "knn_res_df\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.8 Show results of Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>French</th>\n",
       "      <th>German</th>\n",
       "      <th>Polish</th>\n",
       "      <th>Spanish</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>French</th>\n",
       "      <td>0.797659</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>German</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.792085</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Polish</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.867893</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Spanish</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.719064</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           French    German    Polish   Spanish\n",
       "French   0.797659       NaN       NaN       NaN\n",
       "German        NaN  0.792085       NaN       NaN\n",
       "Polish        NaN       NaN  0.867893       NaN\n",
       "Spanish       NaN       NaN       NaN  0.719064"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "per_res = shuffled['Perceptron']\n",
    "data = {key: [result[0] if result[1] == key else None for result in per_res.values()] for key in per_res.keys()}\n",
    "per_res_df = pd.DataFrame(data, index=data.keys())\n",
    "per_res_df.to_csv('../data/perceptron_res_lang_shuffle_4_encoding.csv') # save as csv\n",
    "per_res_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.9 Experiment 3: Train and test on WHOLE dataset as one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_df = encode(reduced_df, 4) #  4 letters encoded\n",
    "X, y = get_X_y(encoded_df) # X is the vector representationf for n amount of letters, y are the labels\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# KNN\n",
    "knn = KNeighborsClassifier(n_neighbors=3) # initialize a KNN class, 3 neighbors\n",
    "knn.fit(X_train, y_train) # train it \n",
    "knn_score = knn.score(X_test, y_test) # get score\n",
    "\n",
    "# Perceptron\n",
    "p = Perceptron(random_state=42) # initialize a Perceptron class, random state 42\n",
    "p.fit(X_train, y_train) # train it\n",
    "p_score = p.score(X_test, y_test) # get score\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.10 result of KNN being trained and tested on whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8321488294314381"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.11 result of Perceptron being trained and tested on whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8342391304347826"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
