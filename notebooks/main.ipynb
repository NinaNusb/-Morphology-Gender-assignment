{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /Users/nazanin/.local/share/virtualenvs/-Morphology-Gender-assignment-61NvD_A8/lib/python3.11/site-packages (3.5.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/nazanin/.local/share/virtualenvs/-Morphology-Gender-assignment-61NvD_A8/lib/python3.11/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/nazanin/.local/share/virtualenvs/-Morphology-Gender-assignment-61NvD_A8/lib/python3.11/site-packages (from spacy) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/nazanin/.local/share/virtualenvs/-Morphology-Gender-assignment-61NvD_A8/lib/python3.11/site-packages (from spacy) (1.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/nazanin/.local/share/virtualenvs/-Morphology-Gender-assignment-61NvD_A8/lib/python3.11/site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/nazanin/.local/share/virtualenvs/-Morphology-Gender-assignment-61NvD_A8/lib/python3.11/site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /Users/nazanin/.local/share/virtualenvs/-Morphology-Gender-assignment-61NvD_A8/lib/python3.11/site-packages (from spacy) (8.1.9)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/nazanin/.local/share/virtualenvs/-Morphology-Gender-assignment-61NvD_A8/lib/python3.11/site-packages (from spacy) (1.1.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/nazanin/.local/share/virtualenvs/-Morphology-Gender-assignment-61NvD_A8/lib/python3.11/site-packages (from spacy) (2.4.6)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/nazanin/.local/share/virtualenvs/-Morphology-Gender-assignment-61NvD_A8/lib/python3.11/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /Users/nazanin/.local/share/virtualenvs/-Morphology-Gender-assignment-61NvD_A8/lib/python3.11/site-packages (from spacy) (0.7.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /Users/nazanin/.local/share/virtualenvs/-Morphology-Gender-assignment-61NvD_A8/lib/python3.11/site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/nazanin/.local/share/virtualenvs/-Morphology-Gender-assignment-61NvD_A8/lib/python3.11/site-packages (from spacy) (6.3.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/nazanin/.local/share/virtualenvs/-Morphology-Gender-assignment-61NvD_A8/lib/python3.11/site-packages (from spacy) (4.65.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/nazanin/.local/share/virtualenvs/-Morphology-Gender-assignment-61NvD_A8/lib/python3.11/site-packages (from spacy) (1.24.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/nazanin/.local/share/virtualenvs/-Morphology-Gender-assignment-61NvD_A8/lib/python3.11/site-packages (from spacy) (2.28.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /Users/nazanin/.local/share/virtualenvs/-Morphology-Gender-assignment-61NvD_A8/lib/python3.11/site-packages (from spacy) (1.10.7)\n",
      "Requirement already satisfied: jinja2 in /Users/nazanin/.local/share/virtualenvs/-Morphology-Gender-assignment-61NvD_A8/lib/python3.11/site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /Users/nazanin/.local/share/virtualenvs/-Morphology-Gender-assignment-61NvD_A8/lib/python3.11/site-packages (from spacy) (67.7.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/nazanin/.local/share/virtualenvs/-Morphology-Gender-assignment-61NvD_A8/lib/python3.11/site-packages (from spacy) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/nazanin/.local/share/virtualenvs/-Morphology-Gender-assignment-61NvD_A8/lib/python3.11/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/nazanin/.local/share/virtualenvs/-Morphology-Gender-assignment-61NvD_A8/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/nazanin/.local/share/virtualenvs/-Morphology-Gender-assignment-61NvD_A8/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/nazanin/.local/share/virtualenvs/-Morphology-Gender-assignment-61NvD_A8/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/nazanin/.local/share/virtualenvs/-Morphology-Gender-assignment-61NvD_A8/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/nazanin/.local/share/virtualenvs/-Morphology-Gender-assignment-61NvD_A8/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/nazanin/.local/share/virtualenvs/-Morphology-Gender-assignment-61NvD_A8/lib/python3.11/site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/nazanin/.local/share/virtualenvs/-Morphology-Gender-assignment-61NvD_A8/lib/python3.11/site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.0.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/nazanin/.local/share/virtualenvs/-Morphology-Gender-assignment-61NvD_A8/lib/python3.11/site-packages (from typer<0.8.0,>=0.3.0->spacy) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/nazanin/.local/share/virtualenvs/-Morphology-Gender-assignment-61NvD_A8/lib/python3.11/site-packages (from jinja2->spacy) (2.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, '../src/')\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from collections import defaultdict\n",
    "from data_cleaning import raw_json_to_clean_df, split_df, df_lang\n",
    "from data_transformation import transform, get_X_y, distribution\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Collection\n",
    "\n",
    "We begin by scrapping Wiktionary.org for *feminine*, *masculine*, and *neuter* nouns in *Polish*, *German*, *Spanish*, and *French*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run webscrapper"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Cleaning\n",
    "\n",
    "The raw data in json format must be cleaned: removing nouns with *spaces*, *hyphens*, *numbers*, *abbreviations*, *initials* and finally those that are *proper nouns*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>noun</th>\n",
       "      <th>gender</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>masculine</td>\n",
       "      <td>Polish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Aalborg</td>\n",
       "      <td>masculine</td>\n",
       "      <td>Polish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aalen</td>\n",
       "      <td>masculine</td>\n",
       "      <td>Polish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Aarhus</td>\n",
       "      <td>masculine</td>\n",
       "      <td>Polish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abacysta</td>\n",
       "      <td>masculine</td>\n",
       "      <td>Polish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328184</th>\n",
       "      <td>zurrona</td>\n",
       "      <td>feminine</td>\n",
       "      <td>Spanish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328185</th>\n",
       "      <td>zutana</td>\n",
       "      <td>feminine</td>\n",
       "      <td>Spanish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328186</th>\n",
       "      <td>zwingliana</td>\n",
       "      <td>feminine</td>\n",
       "      <td>Spanish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328187</th>\n",
       "      <td>zwingliana</td>\n",
       "      <td>feminine</td>\n",
       "      <td>Spanish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328188</th>\n",
       "      <td>zwingliana</td>\n",
       "      <td>feminine</td>\n",
       "      <td>Spanish</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>328189 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              noun     gender     lang\n",
       "0                a  masculine   Polish\n",
       "1          Aalborg  masculine   Polish\n",
       "2            aalen  masculine   Polish\n",
       "3           Aarhus  masculine   Polish\n",
       "4         abacysta  masculine   Polish\n",
       "...            ...        ...      ...\n",
       "328184     zurrona   feminine  Spanish\n",
       "328185      zutana   feminine  Spanish\n",
       "328186  zwingliana   feminine  Spanish\n",
       "328187  zwingliana   feminine  Spanish\n",
       "328188  zwingliana   feminine  Spanish\n",
       "\n",
       "[328189 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read json file and load it as a DataFrame\n",
    "path = '../data/raw_scraped_data.json'\n",
    "raw_df = pd.read_json(path)\n",
    "raw_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>noun</th>\n",
       "      <th>gender</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>723</th>\n",
       "      <td>AC45</td>\n",
       "      <td>feminine</td>\n",
       "      <td>French</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>724</th>\n",
       "      <td>AC72</td>\n",
       "      <td>feminine</td>\n",
       "      <td>French</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>10-Eck</td>\n",
       "      <td>neuter</td>\n",
       "      <td>German</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002</th>\n",
       "      <td>11-Eck</td>\n",
       "      <td>neuter</td>\n",
       "      <td>German</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003</th>\n",
       "      <td>12-Eck</td>\n",
       "      <td>neuter</td>\n",
       "      <td>German</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235408</th>\n",
       "      <td>impresora 3D</td>\n",
       "      <td>feminine</td>\n",
       "      <td>Spanish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299042</th>\n",
       "      <td>TV3</td>\n",
       "      <td>feminine</td>\n",
       "      <td>Spanish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299237</th>\n",
       "      <td>TV3</td>\n",
       "      <td>feminine</td>\n",
       "      <td>Spanish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299437</th>\n",
       "      <td>TV3</td>\n",
       "      <td>feminine</td>\n",
       "      <td>Spanish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299520</th>\n",
       "      <td>TV3</td>\n",
       "      <td>feminine</td>\n",
       "      <td>Spanish</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>125 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                noun    gender     lang\n",
       "723             AC45  feminine   French\n",
       "724             AC72  feminine   French\n",
       "2000          10-Eck    neuter   German\n",
       "2002          11-Eck    neuter   German\n",
       "2003          12-Eck    neuter   German\n",
       "...              ...       ...      ...\n",
       "235408  impresora 3D  feminine  Spanish\n",
       "299042           TV3  feminine  Spanish\n",
       "299237           TV3  feminine  Spanish\n",
       "299437           TV3  feminine  Spanish\n",
       "299520           TV3  feminine  Spanish\n",
       "\n",
       "[125 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_nums = raw_df[raw_df['noun'].str.contains('1|2|3|4|5|6|7|8|9|0')]\n",
    "no_nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(301257, 3)\n",
      "(299192, 3)\n"
     ]
    }
   ],
   "source": [
    "# remove numbers, hypens, spaces, and periods\n",
    "no_nums = raw_df[(~raw_df['noun'].str.contains('-| |\\.|1|2|3|4|5|6|7|8|9|0'))]\n",
    "print(no_nums.shape)\n",
    "# remove full uppercase\n",
    "no_caps = no_nums[(~no_nums['noun'].str.isupper())]\n",
    "print(no_caps.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Break df into each language to: \n",
    "- remove duplicates in each language\n",
    "- remove nouns that begin with a capital letter (except for German) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate df for each language\n",
    "dfs = df_lang(no_caps)\n",
    "\n",
    "# remove nouns that begin with captial letters (except German)\n",
    "temp = []\n",
    "for sub_df in dfs:\n",
    "    if sub_df.lang != 'German':\n",
    "        temp.append(sub_df.df[~sub_df.df['noun'].str.istitle()])\n",
    "    else: # otherwise, for German, pass into SpaCy and filter out Proper nouns\n",
    "        words = pd.Series(sub_df.df['noun']).tolist()\n",
    "        text = \" \".join(words)\n",
    "        nlp_de = spacy.load(\"de_core_news_sm\")\n",
    "        nlp_de.max_length = len(text) \n",
    "        doc = nlp_de(text) \n",
    "        tokens = [token.text for token in doc if token.pos_ != 'PROPN']\n",
    "        temp.append(sub_df.df[sub_df.df['noun'].isin(tokens)])\n",
    "clean = pd.concat(temp)\n",
    "\n",
    "# remove duplicates\n",
    "# no_dups = no_title.drop_duplicates(subset=['noun', 'lang'], keep=False)\n",
    "# print(no_dups.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know want to pass in our dataframe into SpaCy to filter out proper nouns \n",
    "(this should've been acheived on all but German already, but better to be safe than sorry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>noun</th>\n",
       "      <th>gender</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>masculine</td>\n",
       "      <td>Polish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aalen</td>\n",
       "      <td>masculine</td>\n",
       "      <td>Polish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abacysta</td>\n",
       "      <td>masculine</td>\n",
       "      <td>Polish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>abak</td>\n",
       "      <td>masculine</td>\n",
       "      <td>Polish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>abakawir</td>\n",
       "      <td>masculine</td>\n",
       "      <td>Polish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328184</th>\n",
       "      <td>zurrona</td>\n",
       "      <td>feminine</td>\n",
       "      <td>Spanish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328185</th>\n",
       "      <td>zutana</td>\n",
       "      <td>feminine</td>\n",
       "      <td>Spanish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328186</th>\n",
       "      <td>zwingliana</td>\n",
       "      <td>feminine</td>\n",
       "      <td>Spanish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328187</th>\n",
       "      <td>zwingliana</td>\n",
       "      <td>feminine</td>\n",
       "      <td>Spanish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328188</th>\n",
       "      <td>zwingliana</td>\n",
       "      <td>feminine</td>\n",
       "      <td>Spanish</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>261970 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              noun     gender     lang\n",
       "0                a  masculine   Polish\n",
       "2            aalen  masculine   Polish\n",
       "4         abacysta  masculine   Polish\n",
       "5             abak  masculine   Polish\n",
       "7         abakawir  masculine   Polish\n",
       "...            ...        ...      ...\n",
       "328184     zurrona   feminine  Spanish\n",
       "328185      zutana   feminine  Spanish\n",
       "328186  zwingliana   feminine  Spanish\n",
       "328187  zwingliana   feminine  Spanish\n",
       "328188  zwingliana   feminine  Spanish\n",
       "\n",
       "[261970 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Transform and Encode Data\n",
    "reduce data down to an even amount of examples per language and per gender\n",
    "\n",
    "encode last 3 letters of each noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(261970, 3)\n"
     ]
    }
   ],
   "source": [
    "# get length of longest noun\n",
    "max_length = clean['noun'].str.len().max()\n",
    "\n",
    "def add_filler(word):\n",
    "    if len(word) < max_length:\n",
    "        diff = max_length - len(word)\n",
    "        return '#' * diff + word\n",
    "    return word\n",
    "\n",
    "# apply function to every value in column 'noun'\n",
    "\n",
    "clean['noun'] = clean['noun'].apply(add_filler)\n",
    "print(clean.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lowest_value =  2392\n",
      "lang       French  German  Polish   Spanish\n",
      "gender                                     \n",
      "feminine   3801.0  3624.0  2555.0  116375.0\n",
      "masculine  3911.0  2485.0  2681.0  119263.0\n",
      "neuter        NaN  2392.0  4883.0       NaN\n"
     ]
    }
   ],
   "source": [
    "grouped = clean.groupby(['gender','lang']).size().unstack()\n",
    "lowest_value = int(grouped.min().min())\n",
    "print('lowest_value = ', lowest_value)\n",
    "print(distribution(clean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trans(dframe, n=0):\n",
    "    reduced_df = dframe.groupby(['lang', 'gender'])['noun', 'gender', 'lang'].sample(n=lowest_value) # reduce each language and gender by lowest_value\n",
    "    to_be_encoded = reduced_df['noun'].str[-n:] # grab n amount of letters start from the end to encode only\n",
    "    ohe = OneHotEncoder(sparse=False) # initialize the encoder\n",
    "    transformed = ohe.fit_transform(to_be_encoded.to_numpy().reshape(-1, 1)) # encode\n",
    "    transformed_df = pd.DataFrame(transformed) # convert to a dataframe\n",
    "    reduced_df.reset_index(inplace=True, drop=True) # reset indexes\n",
    "    return pd.concat([reduced_df, transformed_df], axis=1) # create new dataframe of reduced df and transformed df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trans_df = trans(clean, 3) # taking only the last three letters\n",
    "# trans_df.head(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Train Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Define X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X, y = get_X_y(trans_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Split data into training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 K-NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# knn = KNeighborsClassifier(n_neighbors=3)\n",
    "# knn.fit(X_train, y_train)\n",
    "# knn.score(X_test, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p = Perceptron(random_state=42)\n",
    "# p.fit(X_train, y_train)\n",
    "# p.score(X_test, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running K-NN and Percetron one by one from last letter until the whole word is encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import defaultdict\n",
    "# def multi_train(df):\n",
    "#     results = defaultdict(list)\n",
    "#     max_length = df['noun'].str.len().max()\n",
    "#     for n in range(1, max_length + 1):\n",
    "#         trans_df = trans(df, n) # taking only n amount of letters\n",
    "#         X, y = get_X_y(trans_df)\n",
    "#         X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "\n",
    "        \n",
    "#         knn = KNeighborsClassifier(n_neighbors=3)\n",
    "#         knn.fit(X_train, y_train)\n",
    "#         results['KNN'].append(knn.score(X_test, y_test))\n",
    "\n",
    "\n",
    "#         p = Perceptron(random_state=42)\n",
    "#         p.fit(X_train, y_train)\n",
    "#         p.score(X_test, y_test)\n",
    "#         results['Perceptron'].append(p.score(X_test, y_test))\n",
    "\n",
    "#     return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res = multi_train(clean)\n",
    "# res"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just Spanish, being trained and tested on Spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_train_per_lang(df):\n",
    "    results = defaultdict(lambda :defaultdict(list))\n",
    "    max_length = df['noun'].str.len().max()\n",
    "    dfs = df_lang(df)\n",
    "    for sub_df in dfs:\n",
    "        for n in range(1, max_length + 1):\n",
    "            trans_df = trans(sub_df.df, n) # taking only n amount of letters\n",
    "            X, y = get_X_y(trans_df)\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "\n",
    "            \n",
    "            knn = KNeighborsClassifier(n_neighbors=3)\n",
    "            knn.fit(X_train, y_train)\n",
    "            results['KNN'][n].append((knn.score(X_test, y_test), sub_df.lang))\n",
    "\n",
    "\n",
    "            p = Perceptron(random_state=42)\n",
    "            p.fit(X_train, y_train)\n",
    "            results['Perceptron'][n].append((p.score(X_test, y_test), sub_df.lang))\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot subset columns with a tuple with more than one element. Use a list instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m all_langs \u001b[39m=\u001b[39m multi_train_per_lang(clean)\n\u001b[1;32m      2\u001b[0m all_langs\n",
      "Cell \u001b[0;32mIn[28], line 7\u001b[0m, in \u001b[0;36mmulti_train_per_lang\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mfor\u001b[39;00m sub_df \u001b[39min\u001b[39;00m dfs:\n\u001b[1;32m      6\u001b[0m     \u001b[39mfor\u001b[39;00m n \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, max_length \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[0;32m----> 7\u001b[0m         trans_df \u001b[39m=\u001b[39m trans(sub_df\u001b[39m.\u001b[39;49mdf, n) \u001b[39m# taking only n amount of letters\u001b[39;00m\n\u001b[1;32m      8\u001b[0m         X, y \u001b[39m=\u001b[39m get_X_y(trans_df)\n\u001b[1;32m      9\u001b[0m         X_train, X_test, y_train, y_test \u001b[39m=\u001b[39m train_test_split(X, y, test_size\u001b[39m=\u001b[39m\u001b[39m0.2\u001b[39m)\n",
      "Cell \u001b[0;32mIn[21], line 2\u001b[0m, in \u001b[0;36mtrans\u001b[0;34m(dframe, n)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrans\u001b[39m(dframe, n\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     reduced_df \u001b[39m=\u001b[39m dframe\u001b[39m.\u001b[39;49mgroupby([\u001b[39m'\u001b[39;49m\u001b[39mlang\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mgender\u001b[39;49m\u001b[39m'\u001b[39;49m])[\u001b[39m'\u001b[39;49m\u001b[39mnoun\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mgender\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mlang\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39msample(n\u001b[39m=\u001b[39mlowest_value) \u001b[39m# reduce each language and gender by lowest_value\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     to_be_encoded \u001b[39m=\u001b[39m reduced_df[\u001b[39m'\u001b[39m\u001b[39mnoun\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mstr[\u001b[39m-\u001b[39mn:] \u001b[39m# grab n amount of letters start from the end to encode only\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     ohe \u001b[39m=\u001b[39m OneHotEncoder(sparse\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m) \u001b[39m# initialize the encoder\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/-Morphology-Gender-assignment-61NvD_A8/lib/python3.11/site-packages/pandas/core/groupby/generic.py:1769\u001b[0m, in \u001b[0;36mDataFrameGroupBy.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1765\u001b[0m \u001b[39m# per GH 23566\u001b[39;00m\n\u001b[1;32m   1766\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, \u001b[39mtuple\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(key) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   1767\u001b[0m     \u001b[39m# if len == 1, then it becomes a SeriesGroupBy and this is actually\u001b[39;00m\n\u001b[1;32m   1768\u001b[0m     \u001b[39m# valid syntax, so don't raise\u001b[39;00m\n\u001b[0;32m-> 1769\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1770\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot subset columns with a tuple with more than one element. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1771\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUse a list instead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1772\u001b[0m     )\n\u001b[1;32m   1773\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__getitem__\u001b[39m(key)\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot subset columns with a tuple with more than one element. Use a list instead."
     ]
    }
   ],
   "source": [
    "all_langs = multi_train_per_lang(clean)\n",
    "all_langs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modified version of the above block\n",
    "def multi_train_scores_per_lang(df):\n",
    "    knn_scores = []\n",
    "    p_scores = []\n",
    "    mlp_scores = []\n",
    "\n",
    "    results = defaultdict(lambda :defaultdict(list))\n",
    "    max_length = df['noun'].str.len().max()\n",
    "    dfs = df_lang(df)\n",
    "    \n",
    "    for sub_df in dfs:\n",
    "        for n in range(1, max_length + 1):\n",
    "            trans_df = trans(sub_df.df, n) # taking only n amount of letters\n",
    "            X, y = get_X_y(trans_df)\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "            # KNN\n",
    "            knn = KNeighborsClassifier(n_neighbors=3)\n",
    "            knn.fit(X_train, y_train)\n",
    "            results['KNN'][n].append((knn.score(X_test, y_test), sub_df.lang))\n",
    "            knn_scores.append(knn.score(X_test, y_test))\n",
    "\n",
    "            # Perceptron\n",
    "            p = Perceptron(random_state=42)\n",
    "            p.fit(X_train, y_train)\n",
    "            results['Perceptron'][n].append((p.score(X_test, y_test), sub_df.lang))\n",
    "            p_scores.append(p.score(X_test, y_test))\n",
    "\n",
    "            # MLP\n",
    "            mlp = MLPClassifier(hidden_layer_sizes=(1500,),\n",
    "                        random_state=42,\n",
    "                        learning_rate_init=0.01)\n",
    "            mlp.fit(X_train,y_train)\n",
    "            mlp_scores.append(mlp.score(X_test, y_test))\n",
    "\n",
    "    return results, knn_scores, p_scores, mlp_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot subset columns with a tuple with more than one element. Use a list instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m (results, knn_scores, p_scores, mlp_scores) \u001b[39m=\u001b[39m multi_train_scores_per_lang(clean)\n",
      "Cell \u001b[0;32mIn[23], line 13\u001b[0m, in \u001b[0;36mmulti_train_scores_per_lang\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39mfor\u001b[39;00m sub_df \u001b[39min\u001b[39;00m dfs:\n\u001b[1;32m     12\u001b[0m     \u001b[39mfor\u001b[39;00m n \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, max_length \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[0;32m---> 13\u001b[0m         trans_df \u001b[39m=\u001b[39m trans(sub_df\u001b[39m.\u001b[39;49mdf, n) \u001b[39m# taking only n amount of letters\u001b[39;00m\n\u001b[1;32m     14\u001b[0m         X, y \u001b[39m=\u001b[39m get_X_y(trans_df)\n\u001b[1;32m     15\u001b[0m         X_train, X_test, y_train, y_test \u001b[39m=\u001b[39m train_test_split(X, y, test_size\u001b[39m=\u001b[39m\u001b[39m0.2\u001b[39m)\n",
      "Cell \u001b[0;32mIn[21], line 2\u001b[0m, in \u001b[0;36mtrans\u001b[0;34m(dframe, n)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrans\u001b[39m(dframe, n\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     reduced_df \u001b[39m=\u001b[39m dframe\u001b[39m.\u001b[39;49mgroupby([\u001b[39m'\u001b[39;49m\u001b[39mlang\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mgender\u001b[39;49m\u001b[39m'\u001b[39;49m])[\u001b[39m'\u001b[39;49m\u001b[39mnoun\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mgender\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mlang\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39msample(n\u001b[39m=\u001b[39mlowest_value) \u001b[39m# reduce each language and gender by lowest_value\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     to_be_encoded \u001b[39m=\u001b[39m reduced_df[\u001b[39m'\u001b[39m\u001b[39mnoun\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mstr[\u001b[39m-\u001b[39mn:] \u001b[39m# grab n amount of letters start from the end to encode only\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     ohe \u001b[39m=\u001b[39m OneHotEncoder(sparse\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m) \u001b[39m# initialize the encoder\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/-Morphology-Gender-assignment-61NvD_A8/lib/python3.11/site-packages/pandas/core/groupby/generic.py:1769\u001b[0m, in \u001b[0;36mDataFrameGroupBy.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1765\u001b[0m \u001b[39m# per GH 23566\u001b[39;00m\n\u001b[1;32m   1766\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, \u001b[39mtuple\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(key) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   1767\u001b[0m     \u001b[39m# if len == 1, then it becomes a SeriesGroupBy and this is actually\u001b[39;00m\n\u001b[1;32m   1768\u001b[0m     \u001b[39m# valid syntax, so don't raise\u001b[39;00m\n\u001b[0;32m-> 1769\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1770\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot subset columns with a tuple with more than one element. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1771\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUse a list instead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1772\u001b[0m     )\n\u001b[1;32m   1773\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__getitem__\u001b[39m(key)\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot subset columns with a tuple with more than one element. Use a list instead."
     ]
    }
   ],
   "source": [
    "(results, knn_scores, p_scores, mlp_scores) = multi_train_scores_per_lang(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Polish</th>\n",
       "      <th>German</th>\n",
       "      <th>French</th>\n",
       "      <th>Spanish</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.940111</td>\n",
       "      <td>0.658774</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.833856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.935933</td>\n",
       "      <td>0.755571</td>\n",
       "      <td>0.772205</td>\n",
       "      <td>0.863114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.852368</td>\n",
       "      <td>0.779248</td>\n",
       "      <td>0.759666</td>\n",
       "      <td>0.809822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.789694</td>\n",
       "      <td>0.755571</td>\n",
       "      <td>0.815047</td>\n",
       "      <td>0.818182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.686630</td>\n",
       "      <td>0.772981</td>\n",
       "      <td>0.770115</td>\n",
       "      <td>0.673981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.754875</td>\n",
       "      <td>0.659471</td>\n",
       "      <td>0.557994</td>\n",
       "      <td>0.609195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.414345</td>\n",
       "      <td>0.598886</td>\n",
       "      <td>0.647858</td>\n",
       "      <td>0.569488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.591226</td>\n",
       "      <td>0.491643</td>\n",
       "      <td>0.547544</td>\n",
       "      <td>0.608150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.471448</td>\n",
       "      <td>0.472841</td>\n",
       "      <td>0.559039</td>\n",
       "      <td>0.551724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.415738</td>\n",
       "      <td>0.369081</td>\n",
       "      <td>0.492163</td>\n",
       "      <td>0.593521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.437326</td>\n",
       "      <td>0.355850</td>\n",
       "      <td>0.536050</td>\n",
       "      <td>0.591432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.378134</td>\n",
       "      <td>0.408774</td>\n",
       "      <td>0.517241</td>\n",
       "      <td>0.513062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.429666</td>\n",
       "      <td>0.451950</td>\n",
       "      <td>0.547544</td>\n",
       "      <td>0.550679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.509749</td>\n",
       "      <td>0.474234</td>\n",
       "      <td>0.524556</td>\n",
       "      <td>0.567398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.489554</td>\n",
       "      <td>0.440111</td>\n",
       "      <td>0.512017</td>\n",
       "      <td>0.502612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.426184</td>\n",
       "      <td>0.436630</td>\n",
       "      <td>0.495298</td>\n",
       "      <td>0.499478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.380223</td>\n",
       "      <td>0.369081</td>\n",
       "      <td>0.563218</td>\n",
       "      <td>0.536050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.364206</td>\n",
       "      <td>0.399025</td>\n",
       "      <td>0.520376</td>\n",
       "      <td>0.584117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.458914</td>\n",
       "      <td>0.362117</td>\n",
       "      <td>0.513062</td>\n",
       "      <td>0.495298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.373259</td>\n",
       "      <td>0.467270</td>\n",
       "      <td>0.547544</td>\n",
       "      <td>0.550679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.489554</td>\n",
       "      <td>0.456825</td>\n",
       "      <td>0.538140</td>\n",
       "      <td>0.516196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.450557</td>\n",
       "      <td>0.389972</td>\n",
       "      <td>0.522466</td>\n",
       "      <td>0.586207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.428273</td>\n",
       "      <td>0.424095</td>\n",
       "      <td>0.522466</td>\n",
       "      <td>0.598746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.367688</td>\n",
       "      <td>0.451253</td>\n",
       "      <td>0.556949</td>\n",
       "      <td>0.526646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.386490</td>\n",
       "      <td>0.412256</td>\n",
       "      <td>0.517241</td>\n",
       "      <td>0.520376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.402507</td>\n",
       "      <td>0.469359</td>\n",
       "      <td>0.504702</td>\n",
       "      <td>0.496343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.459610</td>\n",
       "      <td>0.382312</td>\n",
       "      <td>0.496343</td>\n",
       "      <td>0.577847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.495822</td>\n",
       "      <td>0.422702</td>\n",
       "      <td>0.481714</td>\n",
       "      <td>0.588297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.375348</td>\n",
       "      <td>0.387187</td>\n",
       "      <td>0.492163</td>\n",
       "      <td>0.523511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.520195</td>\n",
       "      <td>0.351671</td>\n",
       "      <td>0.523511</td>\n",
       "      <td>0.516196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.443593</td>\n",
       "      <td>0.391365</td>\n",
       "      <td>0.535005</td>\n",
       "      <td>0.517241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.510446</td>\n",
       "      <td>0.440111</td>\n",
       "      <td>0.527691</td>\n",
       "      <td>0.496343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.373259</td>\n",
       "      <td>0.357242</td>\n",
       "      <td>0.521421</td>\n",
       "      <td>0.525601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.500696</td>\n",
       "      <td>0.458914</td>\n",
       "      <td>0.490073</td>\n",
       "      <td>0.524556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.419220</td>\n",
       "      <td>0.355153</td>\n",
       "      <td>0.509927</td>\n",
       "      <td>0.501567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.385097</td>\n",
       "      <td>0.373955</td>\n",
       "      <td>0.508882</td>\n",
       "      <td>0.482759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.527159</td>\n",
       "      <td>0.472841</td>\n",
       "      <td>0.485893</td>\n",
       "      <td>0.587252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.343315</td>\n",
       "      <td>0.449861</td>\n",
       "      <td>0.523511</td>\n",
       "      <td>0.521421</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Polish    German    French   Spanish\n",
       "1   0.940111  0.658774  0.545455  0.833856\n",
       "2   0.935933  0.755571  0.772205  0.863114\n",
       "3   0.852368  0.779248  0.759666  0.809822\n",
       "4   0.789694  0.755571  0.815047  0.818182\n",
       "5   0.686630  0.772981  0.770115  0.673981\n",
       "6   0.754875  0.659471  0.557994  0.609195\n",
       "7   0.414345  0.598886  0.647858  0.569488\n",
       "8   0.591226  0.491643  0.547544  0.608150\n",
       "9   0.471448  0.472841  0.559039  0.551724\n",
       "10  0.415738  0.369081  0.492163  0.593521\n",
       "11  0.437326  0.355850  0.536050  0.591432\n",
       "12  0.378134  0.408774  0.517241  0.513062\n",
       "13  0.429666  0.451950  0.547544  0.550679\n",
       "14  0.509749  0.474234  0.524556  0.567398\n",
       "15  0.489554  0.440111  0.512017  0.502612\n",
       "16  0.426184  0.436630  0.495298  0.499478\n",
       "17  0.380223  0.369081  0.563218  0.536050\n",
       "18  0.364206  0.399025  0.520376  0.584117\n",
       "19  0.458914  0.362117  0.513062  0.495298\n",
       "20  0.373259  0.467270  0.547544  0.550679\n",
       "21  0.489554  0.456825  0.538140  0.516196\n",
       "22  0.450557  0.389972  0.522466  0.586207\n",
       "23  0.428273  0.424095  0.522466  0.598746\n",
       "24  0.367688  0.451253  0.556949  0.526646\n",
       "25  0.386490  0.412256  0.517241  0.520376\n",
       "26  0.402507  0.469359  0.504702  0.496343\n",
       "27  0.459610  0.382312  0.496343  0.577847\n",
       "28  0.495822  0.422702  0.481714  0.588297\n",
       "29  0.375348  0.387187  0.492163  0.523511\n",
       "30  0.520195  0.351671  0.523511  0.516196\n",
       "31  0.443593  0.391365  0.535005  0.517241\n",
       "32  0.510446  0.440111  0.527691  0.496343\n",
       "33  0.373259  0.357242  0.521421  0.525601\n",
       "34  0.500696  0.458914  0.490073  0.524556\n",
       "35  0.419220  0.355153  0.509927  0.501567\n",
       "36  0.385097  0.373955  0.508882  0.482759\n",
       "37  0.527159  0.472841  0.485893  0.587252\n",
       "38  0.343315  0.449861  0.523511  0.521421"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# idk = pd.DataFrame.from_dict(all_langs, orient=\"index\")\n",
    "# idk\n",
    "knn_results = all_langs['KNN']\n",
    "knn_df = pd.DataFrame.from_dict(knn_results, orient=\"index\", columns=['Polish', 'German', 'French', 'Spanish'])\n",
    "knn_iterative_df = knn_df.applymap(lambda x: x[0])\n",
    "knn_iterative_df.to_csv('../data/knn_results.csv')\n",
    "knn_iterative_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Polish</th>\n",
       "      <th>German</th>\n",
       "      <th>French</th>\n",
       "      <th>Spanish</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.942897</td>\n",
       "      <td>0.371170</td>\n",
       "      <td>0.695925</td>\n",
       "      <td>0.757576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.949164</td>\n",
       "      <td>0.781337</td>\n",
       "      <td>0.829676</td>\n",
       "      <td>0.768025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.952646</td>\n",
       "      <td>0.758357</td>\n",
       "      <td>0.843260</td>\n",
       "      <td>0.823406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.905989</td>\n",
       "      <td>0.821727</td>\n",
       "      <td>0.807732</td>\n",
       "      <td>0.819227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.826602</td>\n",
       "      <td>0.727716</td>\n",
       "      <td>0.766980</td>\n",
       "      <td>0.676071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.665738</td>\n",
       "      <td>0.660864</td>\n",
       "      <td>0.597701</td>\n",
       "      <td>0.611285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.612813</td>\n",
       "      <td>0.599582</td>\n",
       "      <td>0.639498</td>\n",
       "      <td>0.654127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.564067</td>\n",
       "      <td>0.531337</td>\n",
       "      <td>0.547544</td>\n",
       "      <td>0.574713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.512535</td>\n",
       "      <td>0.471448</td>\n",
       "      <td>0.562173</td>\n",
       "      <td>0.551724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.523677</td>\n",
       "      <td>0.503482</td>\n",
       "      <td>0.490073</td>\n",
       "      <td>0.578892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.505571</td>\n",
       "      <td>0.419916</td>\n",
       "      <td>0.513062</td>\n",
       "      <td>0.552769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.490947</td>\n",
       "      <td>0.461003</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.580982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.515320</td>\n",
       "      <td>0.441504</td>\n",
       "      <td>0.494253</td>\n",
       "      <td>0.556949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.509749</td>\n",
       "      <td>0.458217</td>\n",
       "      <td>0.526646</td>\n",
       "      <td>0.567398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.516017</td>\n",
       "      <td>0.468663</td>\n",
       "      <td>0.510972</td>\n",
       "      <td>0.579937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.486769</td>\n",
       "      <td>0.443593</td>\n",
       "      <td>0.505747</td>\n",
       "      <td>0.560084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.500696</td>\n",
       "      <td>0.422006</td>\n",
       "      <td>0.486938</td>\n",
       "      <td>0.576803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.513928</td>\n",
       "      <td>0.474234</td>\n",
       "      <td>0.492163</td>\n",
       "      <td>0.601881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.471448</td>\n",
       "      <td>0.486769</td>\n",
       "      <td>0.514107</td>\n",
       "      <td>0.531870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.493733</td>\n",
       "      <td>0.486769</td>\n",
       "      <td>0.503657</td>\n",
       "      <td>0.552769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.479805</td>\n",
       "      <td>0.540230</td>\n",
       "      <td>0.579937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.509053</td>\n",
       "      <td>0.453343</td>\n",
       "      <td>0.504702</td>\n",
       "      <td>0.586207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.495125</td>\n",
       "      <td>0.497911</td>\n",
       "      <td>0.542320</td>\n",
       "      <td>0.591432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.511142</td>\n",
       "      <td>0.450557</td>\n",
       "      <td>0.501567</td>\n",
       "      <td>0.585162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.474930</td>\n",
       "      <td>0.465181</td>\n",
       "      <td>0.518286</td>\n",
       "      <td>0.579937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.483983</td>\n",
       "      <td>0.457521</td>\n",
       "      <td>0.504702</td>\n",
       "      <td>0.550679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.494429</td>\n",
       "      <td>0.481894</td>\n",
       "      <td>0.526646</td>\n",
       "      <td>0.543365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.496518</td>\n",
       "      <td>0.474234</td>\n",
       "      <td>0.487983</td>\n",
       "      <td>0.588297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.523677</td>\n",
       "      <td>0.465181</td>\n",
       "      <td>0.504702</td>\n",
       "      <td>0.564263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.485376</td>\n",
       "      <td>0.501393</td>\n",
       "      <td>0.525601</td>\n",
       "      <td>0.549634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.518802</td>\n",
       "      <td>0.478412</td>\n",
       "      <td>0.512017</td>\n",
       "      <td>0.613375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.502786</td>\n",
       "      <td>0.476323</td>\n",
       "      <td>0.512017</td>\n",
       "      <td>0.570533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.477019</td>\n",
       "      <td>0.454039</td>\n",
       "      <td>0.516196</td>\n",
       "      <td>0.586207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.501393</td>\n",
       "      <td>0.442897</td>\n",
       "      <td>0.494253</td>\n",
       "      <td>0.590387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.502089</td>\n",
       "      <td>0.472145</td>\n",
       "      <td>0.508882</td>\n",
       "      <td>0.543365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.483287</td>\n",
       "      <td>0.471448</td>\n",
       "      <td>0.501567</td>\n",
       "      <td>0.542320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.480501</td>\n",
       "      <td>0.431058</td>\n",
       "      <td>0.522466</td>\n",
       "      <td>0.587252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.499304</td>\n",
       "      <td>0.472841</td>\n",
       "      <td>0.485893</td>\n",
       "      <td>0.587252</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Polish    German    French   Spanish\n",
       "1   0.942897  0.371170  0.695925  0.757576\n",
       "2   0.949164  0.781337  0.829676  0.768025\n",
       "3   0.952646  0.758357  0.843260  0.823406\n",
       "4   0.905989  0.821727  0.807732  0.819227\n",
       "5   0.826602  0.727716  0.766980  0.676071\n",
       "6   0.665738  0.660864  0.597701  0.611285\n",
       "7   0.612813  0.599582  0.639498  0.654127\n",
       "8   0.564067  0.531337  0.547544  0.574713\n",
       "9   0.512535  0.471448  0.562173  0.551724\n",
       "10  0.523677  0.503482  0.490073  0.578892\n",
       "11  0.505571  0.419916  0.513062  0.552769\n",
       "12  0.490947  0.461003  0.545455  0.580982\n",
       "13  0.515320  0.441504  0.494253  0.556949\n",
       "14  0.509749  0.458217  0.526646  0.567398\n",
       "15  0.516017  0.468663  0.510972  0.579937\n",
       "16  0.486769  0.443593  0.505747  0.560084\n",
       "17  0.500696  0.422006  0.486938  0.576803\n",
       "18  0.513928  0.474234  0.492163  0.601881\n",
       "19  0.471448  0.486769  0.514107  0.531870\n",
       "20  0.493733  0.486769  0.503657  0.552769\n",
       "21  0.500000  0.479805  0.540230  0.579937\n",
       "22  0.509053  0.453343  0.504702  0.586207\n",
       "23  0.495125  0.497911  0.542320  0.591432\n",
       "24  0.511142  0.450557  0.501567  0.585162\n",
       "25  0.474930  0.465181  0.518286  0.579937\n",
       "26  0.483983  0.457521  0.504702  0.550679\n",
       "27  0.494429  0.481894  0.526646  0.543365\n",
       "28  0.496518  0.474234  0.487983  0.588297\n",
       "29  0.523677  0.465181  0.504702  0.564263\n",
       "30  0.485376  0.501393  0.525601  0.549634\n",
       "31  0.518802  0.478412  0.512017  0.613375\n",
       "32  0.502786  0.476323  0.512017  0.570533\n",
       "33  0.477019  0.454039  0.516196  0.586207\n",
       "34  0.501393  0.442897  0.494253  0.590387\n",
       "35  0.502089  0.472145  0.508882  0.543365\n",
       "36  0.483287  0.471448  0.501567  0.542320\n",
       "37  0.480501  0.431058  0.522466  0.587252\n",
       "38  0.499304  0.472841  0.485893  0.587252"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perceptron_results = all_langs['Perceptron']\n",
    "per_df = pd.DataFrame.from_dict(perceptron_results, orient=\"index\", columns=['Polish', 'German', 'French', 'Spanish'])\n",
    "per_iterative_df = per_df.applymap(lambda x: x[0])\n",
    "per_iterative_df.to_csv('../data/perceptron_results.csv')\n",
    "per_iterative_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_train_per_lang_shuffle(df):\n",
    "    results = defaultdict(lambda :defaultdict(list))\n",
    "    # max_length = df['noun'].str.len().max()\n",
    "    max_length = 3\n",
    "    dfs = df_lang(df)\n",
    "    something = defaultdict(lambda: defaultdict(list))\n",
    "    for sub_df in dfs:\n",
    "        for n in range(1, max_length + 1):\n",
    "            trans_df = trans(sub_df.df, n) # taking only n amount of letters\n",
    "            X, y = get_X_y(trans_df)\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "            something[sub_df.lang][n] = [X_train, X_test, y_train, y_test]\n",
    "\n",
    "    # now that we have the X and Y for all iterations, for each language we will fit them\n",
    "\n",
    "    \n",
    "            \n",
    "            # knn = KNeighborsClassifier(n_neighbors=3)\n",
    "            # knn.fit(X_train, y_train)\n",
    "            # results['KNN'][n].append((knn.score(X_test, y_test), sub_df.lang))\n",
    "\n",
    "\n",
    "            # p = Perceptron(random_state=42)\n",
    "            # p.fit(X_train, y_train)\n",
    "            # p.score(X_test, y_test)\n",
    "            # results['Perceptron'][n].append((p.score(X_test, y_test), sub_df.lang))\n",
    "\n",
    "    return results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graphs for each language trained and tested on itself\n",
    "# Plot the accuracies for each dataset\n",
    "classifiers = [\"KNN\", \"Perceptron\", \"MLP\"]\n",
    "x = np.arange(len(classifiers))  # the label locations\n",
    "width = 0.4  # the width of the bars\n",
    "\n",
    "plt.bar(x, knn_scores, width, color=\"DarkSlateGray\", label=\"MLP performance\")        # plot of KNN performance\n",
    "plt.bar(x+width, p_scores, width, color=\"#80b3b3\", label=\"baseline performance\")     # plot of Perceptron performance\n",
    "plt.bar(x+width, mlp_scores, width, color=\"DarkGreen\", label=\"baseline performance\")   # plot of MLP performance\n",
    "plt.title('Accuracy of MLP classifier on different datasets')\n",
    "plt.xlabel('Dataset')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xticks(x+width/2, classifiers)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
